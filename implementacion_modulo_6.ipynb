{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distancias entre textos. El último módulo permitirá, dados dos textos, calcular su distancia semántica. \n",
    "\n",
    "Para ello, evalúa diferentes alternativas y justifica la elección final tomada. \n",
    "La función se denominará texts_distance(text1: str, text2: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_df = pd.read_csv('processed_dataset.csv', encoding='UTF-8', sep=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de un .txt para el manejo de los embeddings con gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_corpus = reddit_df['clean_post'].to_list()\n",
    "\n",
    "with open('clean_post_corpus.txt', 'w', encoding='UTF-8') as f:\n",
    "    for post in post_corpus:\n",
    "        f.write(str(post) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity con TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideal para comparar documentos de diferentes longitudes\n",
    "- Considera la importancia relativa de las palabras\n",
    "- Computacionalmente eficiente\n",
    "- Resultados fáciles de interpretar (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def texts_distance_tfidf(text1: str, text2: str) -> float:\n",
    "    '''\n",
    "    Calcula la distancia entre dos textos usando TfIdf y cosine_similarity\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    return 1 - cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se usó la similitud de coseno como base para medir la distancia. Si dos palabras tienen significados muy similares, su similitud de coseno será cercana a 1, y en consecuencia su distancia (1 - similitud) será cercana a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(text1: str, text2: str) -> float:\n",
    "    '''\n",
    "    Distancia Jaccard entre dos textos. Simple y rápido, aunque solo\n",
    "    considera presencia o ausencia de palabras.\n",
    "    '''\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(tokens2.split())\n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    return 1 - (intersection / union if union != 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cambio de entorno virtual**, uso de gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia basada en Word2Vec\n",
    "\n",
    "La \"Word2Vec distance\" se basa en la representación vectorial de las palabras (embeddings).\n",
    "\n",
    "Word2Vec es un modelo que asigna un vector a cada palabra de manera que palabras similares semánticamente estén cerca en el espacio vectorial.\n",
    "Para medir la similitud (o distancia) entre dos palabras, se calcula por lo general la similitud de coseno entre sus respectivos embeddings.\n",
    "A diferencia de la frecuencia, esta medida tiene en cuenta el contexto semántico de las palabras y no solo la cuenta de apariciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dificultad del manejo de gensim en paralelo con pandas radica en la diferencia de versiones de NumPy que cada librería utiliza (más atrasada en el caso de gensim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Entrenamiento del modelo en el corpus .txt\n",
    "\n",
    "def train_word2vec_model(corpus_path, vector_size=100, min_count=5, workers=4, epochs=10):\n",
    "    \"\"\"\n",
    "    Entrena un modelo Word2Vec usando un corpus de texto línea por línea.\n",
    "    - corpus_path: ruta al archivo .txt con un post/token por línea (o ya tokenizado).\n",
    "    - vector_size: dimensión de los embeddings.\n",
    "    - min_count: ignora palabras con frecuencia < min_count.\n",
    "    - workers: núm. de CPUs o threads a usar.\n",
    "    - epochs: número de pasadas completas sobre el corpus.\n",
    "    \n",
    "    Retorna el modelo entrenado.\n",
    "    \"\"\"\n",
    "    print(\"Cargando corpus y entrenando Word2Vec...\")\n",
    "    sentences = LineSentence(corpus_path)\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    print(\"Entrenamiento finalizado.\")\n",
    "    return model\n",
    "\n",
    "# Función para obtener vector promedio de un texto\n",
    "\n",
    "def get_text_embedding(model, tokenized_text):\n",
    "    \"\"\"\n",
    "    Dado un modelo Word2Vec y una lista de tokens,\n",
    "    retorna el promedio de los vectores de cada token.\n",
    "    Si no se reconoce ningún token, devuelve un vector de ceros.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in tokenized_text:\n",
    "        # Verificamos que el token esté en el vocabulario\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Media de los vectores\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "# Función para calcular la distancia entre dos textos\n",
    "\n",
    "def word2vec_distance(text1_tokens, text2_tokens, model):\n",
    "    \"\"\"\n",
    "    Calcula la similitud y la distancia (1 - similitud_coseno)\n",
    "    entre dos textos tokenizados.\n",
    "    \n",
    "    - text1_tokens: lista de tokens del texto 1.\n",
    "    - text2_tokens: lista de tokens del texto 2.\n",
    "    - model: modelo Word2Vec ya entrenado.\n",
    "    \n",
    "    Retorna la similitud de coseno y la distancia.\n",
    "    \"\"\"\n",
    "    text1_vec = get_text_embedding(model, text1_tokens)\n",
    "    text2_vec = get_text_embedding(model, text2_tokens)\n",
    "    \n",
    "    # Evitar divisiones por cero si algún vector es nulo\n",
    "    if not np.any(text1_vec) or not np.any(text2_vec):\n",
    "        return 0.0, 1.0  # Similitud cero, distancia = 1\n",
    "    \n",
    "    # Similitud de coseno = (A·B) / (||A|| * ||B||)\n",
    "    cos_sim = np.dot(text1_vec, text2_vec) / (norm(text1_vec) * norm(text2_vec))\n",
    "    distance = 1 - cos_sim\n",
    "    \n",
    "    return cos_sim, distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
