{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de resúmenes.  Implementa  un  método  de  resumen  extractivo  de  posts basado  en  frecuencias  y  evalúa  el  resultado.  \n",
    "\n",
    "Deberás  realizar  distintas  pruebas  para \n",
    "demostrar  que  el  método  es  adecuado  para  el  tipo  de  textos,  realizando  los  ajustes \n",
    "necesarios  para  su  correcto  funcionamiento.  \n",
    "\n",
    "La  función  se  denominará \n",
    "post_summarisation(text: str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de resumen extractivo basado en frecuencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beneficios de usar el texto original ('post') a la hora de hacer el resumen:\n",
    "\n",
    "- Preserva la forma exacta del texto\n",
    "\n",
    "- En un resumen extractivo, normalmente se desea mantener intactas las frases que se toman del original para no alterar su significado literal ni su estilo.\n",
    "\n",
    "- Al no lematizar, no se cambian conjugaciones ni palabras, lo que garantiza que el texto sea una cita fiel del documento fuente.\n",
    "\n",
    "- Mayor riqueza lingüística y matices\n",
    "\n",
    "- Las diferencias gramaticales (tiempo verbal, género, número) pueden aportar matices que el lector valora.\n",
    "\n",
    "- Menor riesgo de “romper” la coherencia en frases\n",
    "\n",
    "- Si en un resumen extractivo seleccionamos oraciones tal cual, no necesitamos reconstruir sintácticamente nada.\n",
    "\n",
    "- Dejar las palabras como están minimiza problemas de concordancia (por ejemplo, si “el niño comía” se lematiza y luego sufre transformaciones, podríamos terminar con “el niño comer”, perdiendo coherencia).\n",
    "\n",
    "- Tokenizar (sin lematizar) es más sencillo y rápido; se reduce la complejidad y el coste computacional de la etapa de preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if 'nltk.py' in os.listdir('.') or 'nltk.ipynb' in os.listdir('.'):\n",
    "    print('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Descargar los recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen extractivo:\n",
      "Además, la comunidad de Python es muy activa y proporciona \n",
      "    numerosos recursos para desarrolladores de todos los niveles.\n"
     ]
    }
   ],
   "source": [
    "def post_summarisation_full_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    import string\n",
    "    \"\"\"\n",
    "    Genera un resumen extractivo usando frecuencia de palabras.\n",
    "    text: Texto original (multi-línea) del que se extraerá el resumen.\n",
    "    summary_sentences: Número de oraciones que quieres en tu resumen.\n",
    "    \"\"\"\n",
    "    # Separar en oraciones\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Separar en palabras cada oración para calcular frecuencia\n",
    "    stop_words = set(stopwords.words('english'))  # Ajusta idioma si es necesario\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizar en palabras y limpiar signos de puntuación\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            # Conviertes todo a minúscula y quitas signos de puntuación\n",
    "            word = word.lower()\n",
    "            if word not in stop_words and word not in string.punctuation:\n",
    "                word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "    \n",
    "    # Calcular la frecuencia máxima para normalizar\n",
    "    max_frequency = max(word_frequencies.values()) if word_frequencies else 1\n",
    "    \n",
    "    # Asignar puntuación a cada oración según la frecuencia de sus palabras\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_scores[sentence] = 0\n",
    "        words_in_sentence = word_tokenize(sentence.lower())\n",
    "        for word in words_in_sentence:\n",
    "            if word in word_frequencies:\n",
    "                # Se suma la frecuencia normalizada de la palabra\n",
    "                sentence_scores[sentence] += word_frequencies[word] / max_frequency\n",
    "\n",
    "    # Ordenar oraciones según la puntuación y escoger las mejores\n",
    "    ranked_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "    final_sentences = len(sentences) // 2\n",
    "    summary = \" \".join(ranked_sentences[:final_sentences])\n",
    "    return summary\n",
    "\n",
    "text_example = \"\"\"Aprender a programar requiere práctica y dedicación. Python es un \n",
    "    excelente lenguaje para comenzar debido a su sintaxis sencilla y \n",
    "    versatilidad. Además, la comunidad de Python es muy activa y proporciona \n",
    "    numerosos recursos para desarrolladores de todos los niveles.\"\"\"\n",
    "\n",
    "resumen = post_summarisation_full_text(text_example)\n",
    "print(f\"Resumen extractivo:\")\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen con tokens de la columna 'clean_post' en lugar de 'post'.\n",
    "\n",
    "En esta función tomamos la 'string' de tokens de la columna, calculamos las frecuencias y ordenamos para quedarnos con el 50% de aquellos\n",
    "más comunes. Perdemos legibilidad pero podemos hacernos una mejor idea del tema del texto. \n",
    "\n",
    "```python \n",
    "    # Calcular frecuencias\n",
    "    word_frequencies = {}\n",
    "    for token in token_list:\n",
    "        word_frequencies[token] = word_frequencies.get(token, 0) + 1\n",
    "    \n",
    "    # Ordenar tokens por frecuencia\n",
    "    sorted_tokens = sorted(word_frequencies.items(), \n",
    "                         key=lambda x: x[1], \n",
    "                         reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen (50% de los tokens más frecuentes):\n",
      "python learn program require practice dedication excellent language start due simple syntax versatility community active provide\n"
     ]
    }
   ],
   "source": [
    "def token_based_post_summarisation(tokens: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera un resumen usando el 25% de los tokens más relevantes.\n",
    "    \n",
    "    Args:\n",
    "        tokens (str): String de tokens preprocesados separados por espacios\n",
    "    \n",
    "    Returns:\n",
    "        str: Resumen con los tokens más frecuentes\n",
    "    \"\"\"\n",
    "    # Convertir string de tokens a lista\n",
    "    token_list = tokens.split()\n",
    "    \n",
    "    # Calcular frecuencias\n",
    "    word_frequencies = {}\n",
    "    for token in token_list:\n",
    "        word_frequencies[token] = word_frequencies.get(token, 0) + 1\n",
    "    \n",
    "    # Ordenar tokens por frecuencia\n",
    "    sorted_tokens = sorted(word_frequencies.items(), \n",
    "                         key=lambda x: x[1], \n",
    "                         reverse=True)\n",
    "    \n",
    "    # Seleccionar el 50% de los tokens más frecuentes\n",
    "    num_tokens = max(1, len(token_list) // 2)\n",
    "    selected_tokens = [token for token, _ in sorted_tokens[:num_tokens]]\n",
    "    \n",
    "    return \" \".join(selected_tokens)\n",
    "\n",
    "# Ejemplo de uso\n",
    "clean_tokens_example = \"learn program require practice dedication python excellent language \\\n",
    "    start due simple syntax versatility community python active provide numerous resource developer \\\n",
    "        level beginner find tutorial detailed documentation library python cover practically programming need\"\n",
    "\n",
    "\n",
    "print(\"\\nResumen (50% de los tokens más frecuentes):\")\n",
    "print(token_based_post_summarisation(clean_tokens_example))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
