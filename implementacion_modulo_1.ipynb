{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalización. \n",
    "Implementa una función para limpiar y normalizar los textos de los posts \n",
    "(texto en la columna “post”). \n",
    "\n",
    "La versión normalizada se grabará en una nueva columna \n",
    "“clean_post”. \n",
    "\n",
    "Deberá realizarse una limpieza y normalización de los datos en función de las características que observes en los textos. \n",
    "\n",
    "Describe con detalle cada paso seguido durante el preprocesado y normalización y añade la función preprocess_post(text: str) al fichero \n",
    "core.py con la implementación final de este módulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          created_date created_timestamp  subreddit  \\\n",
      "0  2010-02-11 19:47:22      1265910442.0  analytics   \n",
      "1  2010-03-04 20:17:26      1267726646.0  analytics   \n",
      "2  2011-01-06 04:51:18      1294282278.0  analytics   \n",
      "3  2011-01-19 11:45:30      1295430330.0  analytics   \n",
      "4  2011-01-19 21:52:28      1295466748.0  analytics   \n",
      "\n",
      "                                               title            author  \\\n",
      "0  So what do you guys all do related to analytic...              xtom   \n",
      "1  Google's Invasive, non-Anonymized Ad Targeting...              xtom   \n",
      "2  DotCed - Functional Web Analytics - Tagging, R...            dotced   \n",
      "3            Program Details - Data Analytics Course     iqrconsulting   \n",
      "4  potential job in web analytics... need to anal...  therewontberiots   \n",
      "\n",
      "   author_created_utc                                          full_link  \\\n",
      "0        1.227476e+09  https://www.reddit.com/r/analytics/comments/b0...   \n",
      "1        1.227476e+09  https://www.reddit.com/r/analytics/comments/b9...   \n",
      "2        1.294282e+09  https://www.reddit.com/r/analytics/comments/ew...   \n",
      "3        1.288245e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "4        1.278672e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "\n",
      "   score  num_comments  num_crossposts  subreddit_subscribers  \\\n",
      "0    7.0           4.0             0.0                    NaN   \n",
      "1    2.0           1.0             0.0                    NaN   \n",
      "2    1.0           1.0             NaN                    NaN   \n",
      "3    0.0           0.0             NaN                    NaN   \n",
      "4    2.0           4.0             NaN                    NaN   \n",
      "\n",
      "                                                post sentiment  \n",
      "0  There's a lot of reasons to want to know all t...  NEGATIVE  \n",
      "1  I'm cross posting this from /r/cyberlaw, hopef...  NEGATIVE  \n",
      "2  DotCed,a Functional Analytics Consultant, offe...  NEGATIVE  \n",
      "3  Here is the program details of the data analyt...  NEGATIVE  \n",
      "4  i decided grad school (physics) was not for me...  POSITIVE  \n",
      "Index(['created_date', 'created_timestamp', 'subreddit', 'title', 'author',\n",
      "       'author_created_utc', 'full_link', 'score', 'num_comments',\n",
      "       'num_crossposts', 'subreddit_subscribers', 'post', 'sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'reddit_database_sentiment/reddit_database_sentiment.csv'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    csv_file,\n",
    "    sep=';',\n",
    "    engine='python',\n",
    "    quotechar='\"',\n",
    "    encoding='utf-8',\n",
    "    dtype={\n",
    "    'post': str,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post'].isna().sum()\n",
    "df['post'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peculiaridades del texto para la limpieza y normalización del mismo.\n",
    "\n",
    "Para realizar un análisis de sentimiento sobre el texto de cada post, hay ciertas consideraciones y pasos habituales de preprocesamiento que nosotros seguiremos:\n",
    "\n",
    "1. **Eliminación de caracteres especiales y etiquetas HTML**: \n",
    "\n",
    "A veces el texto podrá contener enlaces, etiquetas HTML o secuencias de caracteres no deseados.\n",
    "\n",
    "Utilizaremos una expresión regular como ```python re.sub(r'<[^>]*>', '', texto)``` para eliminar las etiquetas HTML.\n",
    "\n",
    "2. **Conversión a minúsculas**:\n",
    "\n",
    "Un paso sencillo para normalizar el texto, (p.ej., \"Hola\" y \"hola\" serán considerados como la misma palabra).\n",
    "\n",
    "3. **Eliminación de signos de puntuación**:\n",
    "\n",
    "Para análisis de sentimiento sencillos, a menudo no se tomarán en cuenta estos signos de puntuación.\n",
    "\n",
    "4. **Tokenización**:\n",
    "\n",
    "Dividir el texto en palabras o frases más pequeñas, llamadas \"tokens\".\n",
    "\n",
    "Para ello será utilizada la librería ```nltk``` y su función ```word_tokenize```, como en ```python nltk.word_tokenize(texto)```.\n",
    "\n",
    "5. **Eliminación de stopwords**:\n",
    "\n",
    "Las \"stopwords\" son palabras comunes que no aportan significado al texto, como \"and\", \"the\", \"a\", etc.\n",
    "\n",
    "Para ello utilizaremos la librería ```nltk``` y su función ```stopwords.words('english')```, teniendo en cuenta que el idioma de los posts es el inglés.\n",
    "\n",
    "6. **Lematización o Stemming, según la precisión que se requiera**:\n",
    "\n",
    "Lemmatization y Stemming son técnicas de normalización de texto que buscan reducir las palabras a su raíz o lema. La primera toma en cuenta la gramática y el contexto de la palabra, mientras que la segunda es más sencilla y rápida.\n",
    "\n",
    "Esto sirve para reducir las palabras a su forma de diccionario, agrupando diferentes inflexiones de una misma raíz, y estandarizando el texto.\n",
    "\n",
    "Reduce el tamaño de nuestro vocabulario, reduce la 'sparsity' de nuestros datos y mejora el rendimiento de nuestro modelo.\n",
    "\n",
    "7. **Manejo de negaciones y expresiones idiomáticas**:\n",
    "\n",
    "Para análisis de sentimiento más avanzados, se pueden considerar expresiones idiomáticas y negaciones, como \"not good\".\n",
    "\n",
    "Algunas librerías que podrían ser útiles para este análisis son ```nltk``` y ```textblob```.\n",
    "\n",
    "8. **Corregir espacios de líneas o en blanco**:\n",
    "\n",
    "Puede haber espacios en blanco o líneas vacías que no aporten información al texto. ```re.sub(r'\\s+', ' ', texto)``` o ``` text = text.replace('\\n', ' ').replace('\\r', ' ')``` pueden ser útiles para corregir esto.\t\n",
    "\n",
    "9. **Consideraciones de contexto**:\n",
    "\n",
    "Algunas expresiones en **Reddit** o redes sociales pueden ser sarcásticas, irónicas o usar emojis.\n",
    "\n",
    "El análisis de sentimiento puede no ser preciso en estos casos, y se pueden considerar técnicas más avanzadas de procesamiento de lenguaje natural, como el uso de embeddings contextuales (p. ej., BERT).\n",
    "\n",
    "En resumen, la correcta limpieza y estandarización del texto nos permitirá realizar un \"sentiment analysis\" más preciso, evitando ruidos comunes y reduciendo la complejidad de los datos antes de aplicar técnicas de NLP o modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(nltk\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_post(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags    \n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces between words\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join with single spaces\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prueba de la función</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:  \n",
      "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
      "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
      "Contact me at 123-456-7890. #datascience #machinelearning\n",
      "\n",
      "Texto limpio:  check amazing project machine learning visit details thanks insights also shoutout support contact datascience machinelearning\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
    "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
    "Contact me at 123-456-7890. #datascience #machinelearning\n",
    "\"\"\"\n",
    "clean_text = preprocess_post(test_text)\n",
    "print(\"Texto original: \", test_text)\n",
    "print(\"Texto limpio: \", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion de processed_dataset.csv con el dataframe modificado\n",
    "df.to_csv('processed_dataset.csv', \n",
    "    sep=';',\n",
    "    engine='python',\n",
    "    quotechar='\"',\n",
    "    encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
