{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b58eb3e91594ea8",
   "metadata": {},
   "source": [
    "Implementación y explicación del método para el preprocesado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd86d2e4065dced",
   "metadata": {},
   "source": [
    "Implementa una función llamada preprocess_post(text: str) en el archivo core.py.\n",
    "La función debe limpiar y normalizar los textos de los posts (columna \"post\") y guardar el resultado en una nueva columna \"clean_post\".\n",
    "Describe detalladamente los pasos de preprocesamiento en un notebook de Python (implementacion_modulo_1.ipynb), como eliminación de signos de puntuación, conversión a minúsculas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9029ac8",
   "metadata": {},
   "source": [
    "---**Importamos el dataframe**--- \n",
    "\n",
    "Utilizado posteriormente para columna 'clean_post' y creación del dataset para los siguientes módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7295097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_df = pd.read_csv('reddit_database_sentiment/reddit_database_sentiment.csv', delimiter=';', quotechar='\"', encoding='utf-8', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551b36f",
   "metadata": {},
   "source": [
    "**Análisis de la información de la columna post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "595d76f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm cross posting this from /r/cyberlaw, hopefully you guys find it as interesting as I did(it deals with Google Analytics):\n",
      "\n",
      "So quite awhile ago, I ordered a Papa John's pizza online. My job largely involves looking at ads that appear online, so afterwards I was quick to notice *I was getting a LOT* of Papa Johns ads (especially at night) being served through a Google owned company (DoubleClick media). Yesterday one of these ads popped up again on Youtube (a place that typically serves using the adwords program, not doubleclick), so I decided to copy the URL. \n",
      "\n",
      "For those not in the advertising field: Making full use of Google's analytics tool means that certain information about the advertising campaign is leaked in the URL.\n",
      "\n",
      "So let's break it apart: \n",
      "\n",
      "&gt;http://ad.doubleclick.net/click;h=(junk here);~sscs=?http://googleads.g.doubleclick.net/aclk?sa=l&amp;ai=(junk here)&amp;adurl=http://www.papajohns.com/index.shtm?utm_source=googlenetwork&amp;utm_medium=DisplayCPC&amp;utm_campaign=GoogleRemarketing\n",
      "\n",
      "First off, we see ~sscs: ~sscs is doubleclick's redirect variable. So rather than directly serving adwords ads, they overrode it to serve through doubleclick, then redirect through what would otherwise be an adwords link(http://googleads.g.doubleclick.net). This is tighter integration than is generally seen with adwords/doubleclick.\n",
      "\n",
      "* The interesting part is the end variables utm_source=**googlenetwork**&amp;utm_medium=**DisplayCPC**&amp;utm_campaign=**GoogleRemarketing**\n",
      "\n",
      "* DisplayCPC/googlenetwork - Confirmation that doubleclick is now more finely integrated with adwords.\n",
      "\n",
      "* \"GoogleRemarketing\", huh? Let's take a look at the definition for \"Remarketing\"\n",
      "\n",
      "&gt;Using past campaign information to target a particular message to an audience.\n",
      "\n",
      "While in the past behavioral targetting has largely been based on the sum of your use, this is an interesting(though no doubt more widespread than is known) change in that; explicitly targeting old customers though a *massive* network of sites.\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Just thought I'd put this out there. I'm sure it's not new to a lot of people, but at least to me it was interesting to see concepts like this actually put into practice on such a large scale. \n",
      "\n",
      "-----------------------------\n",
      "\n",
      "PS: I did a quick survey across several thousand domains, and for the record: right now, the most common external resource locations on the internet are(Google owned is bolded):\n",
      "\n",
      "**www.google-analytics.com**\n",
      "\n",
      "**pagead2.googlesyndication.com**\n",
      "\n",
      "**googleads.g.doubleclick.net**\n",
      "\n",
      "edge.quantserve.com\n",
      "\n",
      "**ad.doubleclick.net**\n",
      "\n",
      "**www.youtube.com**\n",
      "\n",
      "b.scorecardresearch.com\n",
      "\n",
      "s0.2mdn.net\n",
      "\n",
      "dg.specificclick.net\n",
      "\n",
      "view.atdmt.com\n",
      "\n",
      "**www.google.com**\n",
      "\n",
      "**ajax.googleapis.com**\n",
      "\n",
      "**partner.googleadservices.com**\n",
      "\n",
      "That's a lot of data.\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df['post'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412175b8",
   "metadata": {},
   "source": [
    "Aquí se observa que es necesario:\n",
    "- Cambiar a minúsculas.\n",
    "- Quitar saltos de líneas y demás letras escapadas ('\\n', '\\r', '\\t', '\\b', '\\f').\n",
    "- Eliminar URLs, que no aportan al análisis de sentimiento.\n",
    "- Eliminar espacios extra.\n",
    "- Eliminar caracteres especiales y números.\n",
    "- Decodificar el html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afb9289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452c926",
   "metadata": {},
   "source": [
    "**Descargas necesarias (descomentar si no se tienen)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d586050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b8bef",
   "metadata": {},
   "source": [
    "**Explicación paso a paso**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec64f99",
   "metadata": {},
   "source": [
    "Conversión a minúsculas: \\\n",
    " Al convertir todo el texto a minúsculas, evitamos duplicados en diferentes casos (por ejemplo, \"Data\" y \"data\" se tratan como la misma palabra).\n",
    " ```python\n",
    "    text = text.lower()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f5445",
   "metadata": {},
   "source": [
    "Eliminación de URL's: \\\n",
    "Las URLs generalmente no aportan información relevante para el análisis de contenido y pueden distraer al modelo.\n",
    "  ```python\n",
    "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519a99c",
   "metadata": {},
   "source": [
    "Eliminación de menciones de usuario y subreddits: \\\n",
    "Las menciones específicas como @usuario o /r/subreddit son específicas y pueden no ser útiles para la clasificación general.\n",
    "```python \n",
    "text = re.sub(r'@\\w+|\\/r\\/\\w+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e33e5",
   "metadata": {},
   "source": [
    "Eliminación de caracteres especiales y números: \\\n",
    "```python\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef32ec8",
   "metadata": {},
   "source": [
    "Eliminación de Números: \\\n",
    "Los números no pueden ser relevantes para el análisis de sentimiento. (Excluyendo en casos de notas o calificaciones)\n",
    "```python\n",
    "text = re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96bc89",
   "metadata": {},
   "source": [
    "Tokenización, stopwords: \\\n",
    "Dividir el texto en tokens (palabras) facilita el procesamiento posterior, las stopwords son marcas gramaticales que no aportan al análisis de sentimiento.\n",
    "```python\n",
    "tokens = text.split()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9e6a6",
   "metadata": {},
   "source": [
    "En primera instancia, se buscaba lematizar el texto para quedarnos solo con la raíz de las palabras, aunque finalmente se decidió no hacerlo para no perder información relevante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4ffdd",
   "metadata": {},
   "source": [
    "**Código completo de la función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89473d30752967f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_post(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia y normaliza el texto de un post de Reddit.\n",
    "\n",
    "    Parámetros:\n",
    "    text (str): Texto original del post.\n",
    "\n",
    "    Retorna:\n",
    "    str: Texto normalizado para análisis.\n",
    "    \"\"\"\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    text = re.sub(r'@\\w+|\\/r\\/\\w+', '', text)\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    clean_text = ' '.join(lemmas)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5abdc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          created_date created_timestamp  subreddit  \\\n",
      "0  2010-02-11 19:47:22      1265910442.0  analytics   \n",
      "1  2010-03-04 20:17:26      1267726646.0  analytics   \n",
      "2  2011-01-06 04:51:18      1294282278.0  analytics   \n",
      "3  2011-01-19 11:45:30      1295430330.0  analytics   \n",
      "4  2011-01-19 21:52:28      1295466748.0  analytics   \n",
      "\n",
      "                                               title            author  \\\n",
      "0  So what do you guys all do related to analytic...              xtom   \n",
      "1  Google's Invasive, non-Anonymized Ad Targeting...              xtom   \n",
      "2  DotCed - Functional Web Analytics - Tagging, R...            dotced   \n",
      "3            Program Details - Data Analytics Course     iqrconsulting   \n",
      "4  potential job in web analytics... need to anal...  therewontberiots   \n",
      "\n",
      "   author_created_utc                                          full_link  \\\n",
      "0        1.227476e+09  https://www.reddit.com/r/analytics/comments/b0...   \n",
      "1        1.227476e+09  https://www.reddit.com/r/analytics/comments/b9...   \n",
      "2        1.294282e+09  https://www.reddit.com/r/analytics/comments/ew...   \n",
      "3        1.288245e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "4        1.278672e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "\n",
      "   score  num_comments  num_crossposts  subreddit_subscribers  \\\n",
      "0    7.0           4.0             0.0                    NaN   \n",
      "1    2.0           1.0             0.0                    NaN   \n",
      "2    1.0           1.0             NaN                    NaN   \n",
      "3    0.0           0.0             NaN                    NaN   \n",
      "4    2.0           4.0             NaN                    NaN   \n",
      "\n",
      "                                                post sentiment  \n",
      "0  There's a lot of reasons to want to know all t...  NEGATIVE  \n",
      "1  I'm cross posting this from /r/cyberlaw, hopef...  NEGATIVE  \n",
      "2  DotCed,a Functional Analytics Consultant, offe...  NEGATIVE  \n",
      "3  Here is the program details of the data analyt...  NEGATIVE  \n",
      "4  i decided grad school (physics) was not for me...  POSITIVE  \n"
     ]
    }
   ],
   "source": [
    "print(reddit_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed765",
   "metadata": {},
   "source": [
    "**Prueba función** \\\n",
    "Se prueba la función con un texto de ejemplo para verificar que el preprocesamiento se realizó correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e32a29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:  \n",
      "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
      "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
      "Contact me at 123-456-7890. #datascience #machinelearning\n",
      "\n",
      "Texto limpio:  check amazing project machine learning visit detail thanks insight also shoutout support contact datascience machinelearning\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
    "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
    "Contact me at 123-456-7890. #datascience #machinelearning\n",
    "\"\"\"\n",
    "clean_text = preprocess_post(test_text)\n",
    "print(\"Texto original: \", test_text)\n",
    "print(\"Texto limpio: \", clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fcd93",
   "metadata": {},
   "source": [
    "**Creación de la columna clean_post y guardado del dataframe**\n",
    "Ahora trabajaremos para adecuar el dataframe su totalidad, creando la columna clean_post y guardando el dataframe en un nuevo archivo csv.\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01659134",
   "metadata": {},
   "source": [
    "**Preparación del dataset**\n",
    "1. Normalizamos las fechas del dataset en caso de que sean necesarias en posteriores módulo, mejorando la calidad de la información.\n",
    "```python\n",
    "reddit_df['created_date'] = pd.to_datetime(reddit_df['created_date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "reddit_df['author_created_date'] = pd.to_datetime(reddit_df['author_created_utc'], unit='s', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3e4b2",
   "metadata": {},
   "source": [
    "\n",
    "2. Aplicamos la función preprocess_post a la columna post.\n",
    "```python\n",
    "reddit_df['clean_post'] = reddit_df['post'].apply(preprocess_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2eb4f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['created_date'] = pd.to_datetime(reddit_df['created_date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "reddit_df['author_created_date'] = pd.to_datetime(reddit_df['author_created_utc'], unit='s', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "253b132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    There's a lot of reasons to want to know all t...\n",
      "1    I'm cross posting this from /r/cyberlaw, hopef...\n",
      "2    DotCed,a Functional Analytics Consultant, offe...\n",
      "3    Here is the program details of the data analyt...\n",
      "4    i decided grad school (physics) was not for me...\n",
      "Name: post, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(reddit_df['post'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a0626",
   "metadata": {},
   "source": [
    "<h1>Solución de problemas y optimización de la función.</h1>\n",
    "\n",
    "Al aplicar la función preprocess llegábamos a un dataframe vacío, y el ritmo de procesamiento de las 274000 filas habría sido muy lento. Por ello, decidimos incluir mejoras para la función (como reducir las veces que se compila cada patrón regex de 1 vez por fila a 1 vez en total), así como para la aplicación en el dataframe.\n",
    "\n",
    "<h2>Mejoras implementadas</h2>\n",
    "\n",
    "-  Importaciones Optimizadas: usamos tqdm.auto e funciones específicas de NLTK para reducir overhead.\n",
    "\n",
    "- Precompilación de Patrones Regex: mejora significativa en velocidad para grandes volúmenes de datos.\n",
    "\n",
    "- Inicialización de Recursos NLTK: carga única de recursos y uso de set() para búsqueda O(1) de stopwords.\n",
    "\n",
    "- Función Principal Optimizada: validación temprana y encadenamiento de operaciones.\n",
    "\n",
    "- Procesamiento por Chunks: mejor gestión de memoria y progreso visible.\n",
    "\n",
    "- Implementación con Control: manejo de errores y métricas de rendimiento.\n",
    "\n",
    "- Estas mejoras han resultado capitales para solucionar nuestros problemas y crear 'processed_dataset' en mucho menos tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9393922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento de 274239 filas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 10000/10000 [00:04<00:00, 2214.09it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:04<00:00, 2048.18it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1891.86it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1925.19it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1838.52it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:03<00:00, 2626.58it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:03<00:00, 2614.06it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1988.12it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1931.14it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1905.23it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1860.15it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1870.36it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1779.85it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1800.13it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1909.41it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1737.26it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1908.18it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1678.59it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1674.26it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1935.52it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:04<00:00, 2205.89it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:03<00:00, 2611.26it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:04<00:00, 2391.88it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:04<00:00, 2084.76it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:06<00:00, 1558.51it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1957.39it/s]\n",
      "Pandas Apply: 100%|██████████| 10000/10000 [00:05<00:00, 1699.73it/s]\n",
      "Pandas Apply: 100%|██████████| 4239/4239 [00:02<00:00, 1712.16it/s]\n",
      "100%|██████████| 28/28 [02:37<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Procesamiento completado:\n",
      "    - Tiempo total: 2.63 minutos\n",
      "    - Velocidad: 104158 filas/minuto\n",
      "    - Filas procesadas: 274239\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Precompilar patrones regex (más eficiente)\n",
    "HTML_PATTERN = re.compile(r'<[^>]*>')\n",
    "URL_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_PATTERN = re.compile(r'@\\w+|#\\w+')\n",
    "SPECIAL_CHARS = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "# 2. Cargar recursos NLTK una sola vez\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def optimized_preprocess(text: str) -> str:\n",
    "    \"\"\"Versión optimizada de preprocess_post\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Aplicar todas las limpiezas de texto en una sola pasada\n",
    "    text = (HTML_PATTERN.sub(' ', text.lower())\n",
    "            .replace('\\n', ' ')\n",
    "            .replace('\\t', ' '))\n",
    "    \n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = MENTION_PATTERN.sub('', text)\n",
    "    text = SPECIAL_CHARS.sub(' ', text)\n",
    "    \n",
    "    # Tokenizar y limpiar en una sola pasada\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in word_tokenize(text)\n",
    "        if token not in stop_words and token.isalnum()\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 3. Procesar en chunks para mejor manejo de memoria\n",
    "def process_in_chunks(df, chunk_size=10000):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):\n",
    "        chunk = df[i:i + chunk_size].copy()\n",
    "        chunk['clean_post'] = chunk['post'].swifter.apply(optimized_preprocess)\n",
    "        results.append(chunk)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# 4. Implementación con control de progreso y manejo de errores\n",
    "try:\n",
    "    print(f\"Iniciando procesamiento de {len(reddit_df)} filas...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Procesar en chunks\n",
    "    processed_df = process_in_chunks(reddit_df)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = (end_time - start_time) / 60\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Procesamiento completado:\n",
    "    - Tiempo total: {processing_time:.2f} minutos\n",
    "    - Velocidad: {len(reddit_df)/processing_time:.0f} filas/minuto\n",
    "    - Filas procesadas: {len(processed_df)}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Guardar resultados periódicamente\n",
    "    processed_df.to_csv('processed_dataset.csv', \n",
    "                       index=False, \n",
    "                       sep=';',\n",
    "                       encoding='utf-8')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error durante el procesamiento: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14ba9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Generar el archivo CSV\n",
    "processed_dataset_path = 'processed_dataset.csv'\n",
    "\n",
    "# Crear un ZIP y añadir el CSV\n",
    "zip_file_name = 'processed_dataset.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(processed_dataset_path, arcname='processed_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
