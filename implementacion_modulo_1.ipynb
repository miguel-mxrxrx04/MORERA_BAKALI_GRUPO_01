{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalización. \n",
    "Implementa una función para limpiar y normalizar los textos de los posts \n",
    "(texto en la columna “post”). \n",
    "\n",
    "La versión normalizada se grabará en una nueva columna \n",
    "“clean_post”. \n",
    "\n",
    "Deberá realizarse una limpieza y normalización de los datos en función de las características que observes en los textos. \n",
    "\n",
    "Describe con detalle cada paso seguido durante el preprocesado y normalización y añade la función preprocess_post(text: str) al fichero \n",
    "core.py con la implementación final de este módulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          created_date created_timestamp  subreddit  \\\n",
      "0  2010-02-11 19:47:22      1265910442.0  analytics   \n",
      "1  2010-03-04 20:17:26      1267726646.0  analytics   \n",
      "2  2011-01-06 04:51:18      1294282278.0  analytics   \n",
      "3  2011-01-19 11:45:30      1295430330.0  analytics   \n",
      "4  2011-01-19 21:52:28      1295466748.0  analytics   \n",
      "\n",
      "                                               title            author  \\\n",
      "0  So what do you guys all do related to analytic...              xtom   \n",
      "1  Google's Invasive, non-Anonymized Ad Targeting...              xtom   \n",
      "2  DotCed - Functional Web Analytics - Tagging, R...            dotced   \n",
      "3            Program Details - Data Analytics Course     iqrconsulting   \n",
      "4  potential job in web analytics... need to anal...  therewontberiots   \n",
      "\n",
      "   author_created_utc                                          full_link  \\\n",
      "0        1.227476e+09  https://www.reddit.com/r/analytics/comments/b0...   \n",
      "1        1.227476e+09  https://www.reddit.com/r/analytics/comments/b9...   \n",
      "2        1.294282e+09  https://www.reddit.com/r/analytics/comments/ew...   \n",
      "3        1.288245e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "4        1.278672e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
      "\n",
      "   score  num_comments  num_crossposts  subreddit_subscribers  \\\n",
      "0    7.0           4.0             0.0                    NaN   \n",
      "1    2.0           1.0             0.0                    NaN   \n",
      "2    1.0           1.0             NaN                    NaN   \n",
      "3    0.0           0.0             NaN                    NaN   \n",
      "4    2.0           4.0             NaN                    NaN   \n",
      "\n",
      "                                                post sentiment  \n",
      "0  There's a lot of reasons to want to know all t...  NEGATIVE  \n",
      "1  I'm cross posting this from /r/cyberlaw, hopef...  NEGATIVE  \n",
      "2  DotCed,a Functional Analytics Consultant, offe...  NEGATIVE  \n",
      "3  Here is the program details of the data analyt...  NEGATIVE  \n",
      "4  i decided grad school (physics) was not for me...  POSITIVE  \n",
      "Index(['created_date', 'created_timestamp', 'subreddit', 'title', 'author',\n",
      "       'author_created_utc', 'full_link', 'score', 'num_comments',\n",
      "       'num_crossposts', 'subreddit_subscribers', 'post', 'sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'reddit_database_sentiment/reddit_database_sentiment.csv'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    csv_file,\n",
    "    sep=';',\n",
    "    engine='python',\n",
    "    quotechar='\"',\n",
    "    encoding='utf-8',\n",
    "    dtype={\n",
    "    'post': str,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post'].isna().sum()\n",
    "df['post'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peculiaridades del texto para la limpieza y normalización del mismo.\n",
    "\n",
    "Para realizar un análisis de sentimiento sobre el texto de cada post, hay ciertas consideraciones y pasos habituales de preprocesamiento que nosotros seguiremos:\n",
    "\n",
    "1. **Eliminación de caracteres especiales y etiquetas HTML**: \n",
    "\n",
    "A veces el texto podrá contener enlaces, etiquetas HTML o secuencias de caracteres no deseados.\n",
    "\n",
    "Utilizaremos una expresión regular como ```python re.sub(r'<[^>]*>', '', texto)``` para eliminar las etiquetas HTML.\n",
    "\n",
    "2. **Conversión a minúsculas**:\n",
    "\n",
    "Un paso sencillo para normalizar el texto, (p.ej., \"Hola\" y \"hola\" serán considerados como la misma palabra).\n",
    "\n",
    "3. **Eliminación de signos de puntuación**:\n",
    "\n",
    "Para análisis de sentimiento sencillos, a menudo no se tomarán en cuenta estos signos de puntuación.\n",
    "\n",
    "4. **Tokenización**:\n",
    "\n",
    "Dividir el texto en palabras o frases más pequeñas, llamadas \"tokens\".\n",
    "\n",
    "Para ello será utilizada la librería ```nltk``` y su función ```word_tokenize```, como en ```python nltk.word_tokenize(texto)```.\n",
    "\n",
    "5. **Eliminación de stopwords**:\n",
    "\n",
    "Las \"stopwords\" son palabras comunes que no aportan significado al texto, como \"and\", \"the\", \"a\", etc.\n",
    "\n",
    "Para ello utilizaremos la librería ```nltk``` y su función ```stopwords.words('english')```, teniendo en cuenta que el idioma de los posts es el inglés.\n",
    "\n",
    "6. **Lematización o Stemming, según la precisión que se requiera**:\n",
    "\n",
    "Lemmatization y Stemming son técnicas de normalización de texto que buscan reducir las palabras a su raíz o lema. La primera toma en cuenta la gramática y el contexto de la palabra, mientras que la segunda es más sencilla y rápida.\n",
    "\n",
    "Esto sirve para reducir las palabras a su forma de diccionario, agrupando diferentes inflexiones de una misma raíz, y estandarizando el texto.\n",
    "\n",
    "Reduce el tamaño de nuestro vocabulario, reduce la 'sparsity' de nuestros datos y mejora el rendimiento de nuestro modelo.\n",
    "\n",
    "7. **Manejo de negaciones y expresiones idiomáticas**:\n",
    "\n",
    "Para análisis de sentimiento más avanzados, se pueden considerar expresiones idiomáticas y negaciones, como \"not good\".\n",
    "\n",
    "Algunas librerías que podrían ser útiles para este análisis son ```nltk``` y ```textblob```.\n",
    "\n",
    "8. **Corregir espacios de líneas o en blanco**:\n",
    "\n",
    "Puede haber espacios en blanco o líneas vacías que no aporten información al texto. ```re.sub(r'\\s+', ' ', texto)``` o ``` text = text.replace('\\n', ' ').replace('\\r', ' ')``` pueden ser útiles para corregir esto.\t\n",
    "\n",
    "9. **Consideraciones de contexto**:\n",
    "\n",
    "Algunas expresiones en **Reddit** o redes sociales pueden ser sarcásticas, irónicas o usar emojis.\n",
    "\n",
    "El análisis de sentimiento puede no ser preciso en estos casos, y se pueden considerar técnicas más avanzadas de procesamiento de lenguaje natural, como el uso de embeddings contextuales (p. ej., BERT).\n",
    "\n",
    "En resumen, la correcta limpieza y estandarización del texto nos permitirá realizar un \"sentiment analysis\" más preciso, evitando ruidos comunes y reduciendo la complejidad de los datos antes de aplicar técnicas de NLP o modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Currito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Currito/nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Currito\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[0;32m     29\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello this is a test. I am testing this function. I hope it works. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m <a href=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Google</a>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpreprocess_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m, in \u001b[0;36mpreprocess_post\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     14\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m---> 16\u001b[0m sent_tokens \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# First tokenize\u001b[39;00m\n\u001b[0;32m     19\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(sent_tokens[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sent_tokens)))\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Currito\\Desktop\\workspace\\MORERA_BAKALI_GRUPO_01\\mainenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Currito/nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Currito\\\\Desktop\\\\workspace\\\\MORERA_BAKALI_GRUPO_01\\\\mainenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Currito\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_post(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "        \n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Then filter stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Finally lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "text = 'Hello this is a test. I am testing this function. I hope it works. \\n <a href=\"https://www.google.com\">Google</a>'\n",
    "print(preprocess_post(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion de processed_dataset.csv con el dataframe modificado\n",
    "df.to_csv('processed_dataset.csv', \n",
    "    sep=';',\n",
    "    engine='python',\n",
    "    quotechar='\"',\n",
    "    encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
