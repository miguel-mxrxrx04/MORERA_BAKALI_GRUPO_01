{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b58eb3e91594ea8",
   "metadata": {},
   "source": [
    "Implementación y explicación del método para el preprocesado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd86d2e4065dced",
   "metadata": {},
   "source": [
    "Implementa una función llamada preprocess_post(text: str) en el archivo core.py.\n",
    "La función debe limpiar y normalizar los textos de los posts (columna \"post\") y guardar el resultado en una nueva columna \"clean_post\".\n",
    "Describe detalladamente los pasos de preprocesamiento en un notebook de Python (implementacion_modulo_1.ipynb), como eliminación de signos de puntuación, conversión a minúsculas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9029ac8",
   "metadata": {},
   "source": [
    "---**Importamos el dataframe**--- \n",
    "Utilizado posteriormente para columna 'clean_post' y creación del dataframe para losw siguientes módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad5de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.0.2 in c:\\users\\mam\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==\"2.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7295097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\MAM\\AppData\\Local\\Temp\\ipykernel_33476\\109888410.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m reddit_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreddit_database_sentiment/reddit_database_sentiment.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MAM\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_df = pd.read_csv('reddit_database_sentiment/reddit_database_sentiment.csv', delimiter=';', quotechar='\"', encoding='utf-8', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551b36f",
   "metadata": {},
   "source": [
    "**Análisis de la información de la columna post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "595d76f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m cross posting this from /r/cyberlaw, hopefully you guys find it as interesting as I did(it deals with Google Analytics):\\n\\nSo quite awhile ago, I ordered a Papa John\\'s pizza online. My job largely involves looking at ads that appear online, so afterwards I was quick to notice *I was getting a LOT* of Papa Johns ads (especially at night) being served through a Google owned company (DoubleClick media). Yesterday one of these ads popped up again on Youtube (a place that typically serves using the adwords program, not doubleclick), so I decided to copy the URL. \\n\\nFor those not in the advertising field: Making full use of Google\\'s analytics tool means that certain information about the advertising campaign is leaked in the URL.\\n\\nSo let\\'s break it apart: \\n\\n&gt;http://ad.doubleclick.net/click;h=(junk here);~sscs=?http://googleads.g.doubleclick.net/aclk?sa=l&amp;ai=(junk here)&amp;adurl=http://www.papajohns.com/index.shtm?utm_source=googlenetwork&amp;utm_medium=DisplayCPC&amp;utm_campaign=GoogleRemarketing\\n\\nFirst off, we see ~sscs: ~sscs is doubleclick\\'s redirect variable. So rather than directly serving adwords ads, they overrode it to serve through doubleclick, then redirect through what would otherwise be an adwords link(http://googleads.g.doubleclick.net). This is tighter integration than is generally seen with adwords/doubleclick.\\n\\n* The interesting part is the end variables utm_source=**googlenetwork**&amp;utm_medium=**DisplayCPC**&amp;utm_campaign=**GoogleRemarketing**\\n\\n* DisplayCPC/googlenetwork - Confirmation that doubleclick is now more finely integrated with adwords.\\n\\n* \"GoogleRemarketing\", huh? Let\\'s take a look at the definition for \"Remarketing\"\\n\\n&gt;Using past campaign information to target a particular message to an audience.\\n\\nWhile in the past behavioral targetting has largely been based on the sum of your use, this is an interesting(though no doubt more widespread than is known) change in that; explicitly targeting old customers though a *massive* network of sites.\\n\\n-----------------------------------\\n\\nJust thought I\\'d put this out there. I\\'m sure it\\'s not new to a lot of people, but at least to me it was interesting to see concepts like this actually put into practice on such a large scale. \\n\\n-----------------------------\\n\\nPS: I did a quick survey across several thousand domains, and for the record: right now, the most common external resource locations on the internet are(Google owned is bolded):\\n\\n**www.google-analytics.com**\\n\\n**pagead2.googlesyndication.com**\\n\\n**googleads.g.doubleclick.net**\\n\\nedge.quantserve.com\\n\\n**ad.doubleclick.net**\\n\\n**www.youtube.com**\\n\\nb.scorecardresearch.com\\n\\ns0.2mdn.net\\n\\ndg.specificclick.net\\n\\nview.atdmt.com\\n\\n**www.google.com**\\n\\n**ajax.googleapis.com**\\n\\n**partner.googleadservices.com**\\n\\nThat\\'s a lot of data.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['post'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412175b8",
   "metadata": {},
   "source": [
    "Aquí se observa que es necesario:\n",
    "- Cambiar a minúsculas.\n",
    "- Quitar saltos de líneas y demás letras escapadas ('\\n', '\\r', '\\t', '\\b', '\\f').\n",
    "- Eliminar URLs, que no aportan al análisis de sentimiento.\n",
    "- Eliminar espacios extra.\n",
    "- Eliminar caracteres especiales y números.\n",
    "- Decodificar el html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afb9289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452c926",
   "metadata": {},
   "source": [
    "**Descargas necesarias (descomentar si no se tienen)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d586050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b8bef",
   "metadata": {},
   "source": [
    "**Explicación paso a paso**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec64f99",
   "metadata": {},
   "source": [
    "Conversión a minúsculas: \\\n",
    " Al convertir todo el texto a minúsculas, evitamos duplicados en diferentes casos (por ejemplo, \"Data\" y \"data\" se tratan como la misma palabra).\n",
    " ```python\n",
    "    text = text.lower()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f5445",
   "metadata": {},
   "source": [
    "Eliminación de URL's: \\\n",
    "Las URLs generalmente no aportan información relevante para el análisis de contenido y pueden distraer al modelo.\n",
    "  ```python\n",
    "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519a99c",
   "metadata": {},
   "source": [
    "Eliminación de menciones de usuario y subreddits: \\\n",
    "Las menciones específicas como @usuario o /r/subreddit son específicas y pueden no ser útiles para la clasificación general.\n",
    "```python \n",
    "text = re.sub(r'@\\w+|\\/r\\/\\w+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e33e5",
   "metadata": {},
   "source": [
    "Eliminación de caracteres especiales y números: \\\n",
    "```python\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef32ec8",
   "metadata": {},
   "source": [
    "Eliminación de Números: \\\n",
    "Los números no pueden ser relevantes para el análisis de sentimiento. (Excluyendo en casos de notas o calificaciones)\n",
    "```python\n",
    "text = re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96bc89",
   "metadata": {},
   "source": [
    "Tokenización, stopwords: \\\n",
    "Dividir el texto en tokens (palabras) facilita el procesamiento posterior, las stopwords son marcas gramaticales que no aportan al análisis de sentimiento.\n",
    "```python\n",
    "tokens = text.split()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9e6a6",
   "metadata": {},
   "source": [
    "En primera instancia, se buscaba lematizar el texto para quedarnos solo con la raíz de las palabras, aunque finalmente se decidió no hacerlo para no perder información relevante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4ffdd",
   "metadata": {},
   "source": [
    "**Código completo de la función**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89473d30752967f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia y normaliza el texto de un post de Reddit.\n",
    "\n",
    "    Parámetros:\n",
    "    text (str): Texto original del post.\n",
    "\n",
    "    Retorna:\n",
    "    str: Texto normalizado para análisis.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    text = re.sub(r'@\\w+|\\/r\\/\\w+', '', text)\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    clean_text = ' '.join(tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fed765",
   "metadata": {},
   "source": [
    "**Prueba función** \\\n",
    "Se prueba la función con un texto de ejemplo para verificar que el preprocesamiento se realizó correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e32a29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:  \n",
      "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
      "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
      "Contact me at 123-456-7890. #datascience #machinelearning\n",
      "\n",
      "Texto limpio:  Check amazing project machine learning Visit detail Thanks insight Also shoutout support Contact datascience machinelearning\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "Check out this amazing project on machine learning! Visit https://github.com/user/repo for more details.\n",
    "Thanks @datascientist for the insights. Also, shoutout to /r/MachineLearning for the support.\n",
    "Contact me at 123-456-7890. #datascience #machinelearning\n",
    "\"\"\"\n",
    "clean_text = preprocess_post(test_text)\n",
    "print(\"Texto original: \", test_text)\n",
    "print(\"Texto limpio: \", clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fcd93",
   "metadata": {},
   "source": [
    "**Creación de la columna clean_post y guardado del dataframe**\n",
    "Ahora trabajaremos para adecuar el dataframe su totalidad, creando la columna clean_post y guardando el dataframe en un nuevo archivo csv.\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01659134",
   "metadata": {},
   "source": [
    "**Preparación del dataset**\n",
    "1. Normalizamos las fechas del dataset en caso de que sean necesarias en posteriores módulo, mejorando la calidad de la información.\n",
    "```python\n",
    "reddit_df['created_date'] = pd.to_datetime(reddit_df['created_date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "reddit_df['author_created_date'] = pd.to_datetime(reddit_df['author_created_utc'], unit='s', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b50406",
   "metadata": {},
   "source": [
    "2. Llenamos los valores nulos con un string vacío para evitar problemas en el preprocesamiento.\n",
    "```python\n",
    "reddit_df['post'] = reddit_df['post'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3e4b2",
   "metadata": {},
   "source": [
    "\n",
    "3. Aplicamos la función preprocess_post a la columna post.\n",
    "```python\n",
    "reddit_df['clean_post'] = reddit_df['post'].apply(preprocess_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2eb4f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['created_date'] = pd.to_datetime(reddit_df['created_date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "reddit_df['author_created_date'] = pd.to_datetime(reddit_df['author_created_utc'], unit='s', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f060590",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['post'] = reddit_df['post'].fillna('')\n",
    "\n",
    "reddit_df = reddit_df.drop_duplicates()\n",
    "reddit_df = reddit_df.dropna()\n",
    "\n",
    "reddit_df['clean_post'] = reddit_df['post'].apply(preprocess_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14ba9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_path = 'processed_dataset.csv'\n",
    "reddit_df.to_csv(processed_dataset_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4630bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('processed_dataset.csv.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(processed_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
