{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de sentimiento. Implementa un módulo de detección de sentimiento de los posts. \n",
    "\n",
    "Para entrenar y evaluar diferentes métodos, dispones de la columna “sentiment” en el dataset.  \n",
    "Al  igual  que  en  el  módulo  de  clasificación  de  subreddits,  Para  evaluar  cada método, se utilizará la métrica f1 score, y se utilizará un 70% de los datos del dataset para entrenamiento y un 30% para test (realizando un sampling aleatorio previo). Deberás probar \n",
    "y evaluar los siguientes métodos: \n",
    "\n",
    "- Un método basado en lexicons \n",
    "- Un método basado en palabras únicas en textos de cada tipo de sentimiento \n",
    "- Un método basado en Word embeddings + algoritmo de machine learning de clasificación.\n",
    "\n",
    "La función de este módulo tendrá como nombre sentiment_analysis(text: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método basado en lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un método basado en lexicones, no hay un proceso de entrenamiento supervisado convencional; en su lugar, se hace uso de listas o diccionarios de palabras con polaridad (positivas/negativas).\n",
    "\n",
    "El f1 score se calcula, en este caso, asumiendo un problema binario (positivo/negativo) y se utiliza una división de datos del 70% para entrenamiento y 30% para prueba de manera aleatoria, siguiendo tus requerimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_lexicons(text: str):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método basado en palabras únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_bow(text:str):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método basado en word embeddgins + ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias, y extraemos unas muestras para entrenamiento y testing para poder ejecutar el código en un tiempo razonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = reddit_df['clean_post']\n",
    "y = reddit_df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_emb = X_train.sample(frac=0.1, random_state=42)\n",
    "y_train_emb = y_train.loc[X_train_emb.index]\n",
    "X_test_emb = X_test.sample(frac=0.01, random_state=42)\n",
    "y_test_emb = y_test.loc[X_test_emb.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la formación de los embeddins usaremos el modelo distilbert, cuya arquitectura deriva del modelo Bert, pero más simplificado, haciéndolo mucho más rápido sacrificando precisión. Usaremos un modelo preentrenado de la librería transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar DistilBERT y el tokenizador\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la una clase que herede de las clases BaseEstimator y TransformerMixin de la librería sklearn, para que el modelo sea compatible con los pipeline de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No se necesita ajuste en este transformador\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for text in X:\n",
    "                inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                outputs = self.model(**inputs)\n",
    "                token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "                word_embedding = token_embeddings.mean(dim=0).numpy()\n",
    "                embeddings.append(word_embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_lr_pipeline = Pipeline([('vectorizer',  DistilBERTEmbeddingTransformer()), ('logistic', LogisticRegression())])\n",
    "embedding_lr_pipeline.fit(X_train_emb, y_train_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las predicciones con el conjunte de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_embedding_lr = embedding_lr_pipeline.predict(X_test_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos las pruebas de f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_tfidf_lr = f1_score(y_test_emb, y_pred_embedding_lr, average='weighted')\n",
    "print(f'F1 score: {f1_score_tfidf_lr}')\n",
    "print(f'Classification report: {classification_report(y_test, y_pred_tfidf_lr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification report: {classification_report(y_test, y_pred_tfidf_lr)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
