{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "2. Sistema de clasificación de subreddit. \n",
    "\n",
    "Se deberá implementar una función ```classify_subreddit(text)``` que clasifique un texto de entrada en una de las siguientes \n",
    "categorías: **MachineLearning, datascience, statistics, learnmachinelearning, computerscience, AskStatistics, artificial, analytics, datasets, deeplearning, rstats, computervision, DataScienceJobs, MLQuestions, dataengineering, data, dataanalysis, \n",
    "datascienceproject, Kaggle**.\n",
    "\n",
    "Para ello, se proporciona un dataset sobre el que se podrán entrenar distintos algoritmos\n",
    "de clasificación. La etiqueta del subreddit correspondiente se encuentra en la columna \"subreddit\". Se deberán probar, al menos, los siguientes 3 métodos:\n",
    "- Un método basado en TF-IDF + algoritmo de clasificación de machine learning\n",
    "- Un método basado en entidades reconocidas (Named-Entity Recognition) + algoritmo declasificación de machine learning \n",
    "- Un método basado en Word Embeddings + algoritmo de clasificación de machine learning\n",
    "\n",
    "Para evaluar cada método, se utilizará la métrica f1 score, y se utilizará un 70% de los datos\n",
    "del dataset para entrenamiento y un 30% para test (realizando un sampling aleatorio\n",
    "previo).\"\"\"\n",
    "\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(ejercicio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los posts de entrenamiento se encuentran en la columna 'clean_post' del dataframe, y las labels en la columna 'subreddit'. Las siguientes son las categorías en las que clasificaremos cada post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['MachineLearning', 'datascience', 'statistics', 'learnmachinelearning', 'computerscience', 'AskStatistics', 'artificial', 'analytics', 'datasets', 'deeplearning', 'rstats', 'computervision', 'DataScienceJobs', 'MLQuestions', 'dataengineering', 'data', 'dataanalysis', 'datascienceproject', 'Kaggle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reddit_df = pd.read_csv('processed_dataset.csv', sep=';', encoding='UTF-8')\n",
    "\n",
    "# Tomamos el 10% de los datos de manera aleatoria\n",
    "reduced_df = reddit_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Dividimos en entrenamiento (70%) y prueba (30%)\n",
    "# Asumimos que la columna de texto es 'clean_post' y la de sentimientos es 'sentiment'\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_df['clean_post'],\n",
    "    reduced_df['sentiment'],\n",
    "    train_size=0.7,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método basado en TF-IDF + algoritmo de clasificación de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Método TF-IDF + Naive Bayes - F1 Score: 0.2213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# (1) Ajustar TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english'  # si aplica\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# (2) Probar Naive Bayes con alpha personalizado\n",
    "clf_tfidf_nb = MultinomialNB(alpha=0.5)\n",
    "clf_tfidf_nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_nb = clf_tfidf_nb.predict(X_test_tfidf)\n",
    "f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
    "print(f\"Método TF-IDF + Naive Bayes (alpha=0.5) - F1 Score: {f1_nb:.4f}\")\n",
    "\n",
    "# (3) Probar LogisticRegression (opcional)\n",
    "clf_tfidf_lr = LogisticRegression(max_iter=200)\n",
    "clf_tfidf_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_lr = clf_tfidf_lr.predict(X_test_tfidf)\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "print(f\"Método TF-IDF + Logistic Regression - F1 Score: {f1_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un método basado en entidades reconocidas (Named-Entity Recognition) + algoritmo declasificación de machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_ner_features(doc):\n",
    "    entity_counts = {}\n",
    "    for ent in doc.ents:\n",
    "        ent_label = ent.label_\n",
    "        entity_counts[ent_label] = entity_counts.get(ent_label, 0) + 1\n",
    "    return entity_counts\n",
    "\n",
    "def ner_features_from_texts(texts):\n",
    "    features_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un método basado en Word Embeddings + algoritmo de clasificación de machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar incompatibilidades, creamos un dataframe aparte solo con las columnas 'clean_post' y 'subreddit', para posteriormente leerlo en el entrenamiento con Word2Vec de forma más cómo, se guardará 'reddit_posts_reduced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = reddit_df[['clean_post', 'subreddit']]\n",
    "df_reduced.to_csv('reddit_posts_reduced.csv', encoding='UTF-8', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
