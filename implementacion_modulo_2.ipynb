{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Clasifición de textos."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42e6e72bc401a2"
  },
  {
   "cell_type": "markdown",
   "id": "ca3d8727daea264d",
   "metadata": {},
   "source": [
    "Implementa una función llamada classify_subreddit(text: str) que clasifique un texto en una de las categorías de subreddits especificadas.\n",
    "Debes probar al menos 3 métodos:\n",
    "Método basado en TF-IDF + algoritmo de machine learning.\n",
    "Método basado en el reconocimiento de entidades nombradas (NER) + machine learning.\n",
    "Método basado en Word Embeddings + machine learning.\n",
    "Evalúa estos métodos utilizando la métrica f1 score y una división de datos (70% para entrenamiento, 30% para test).\n",
    "Incluye la implementación en core.py y documenta los pasos en implementacion_modulo_2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db15782",
   "metadata": {},
   "source": [
    "Nota: desde la implementacion del modulo 1 vemos que solo deberemos usar como texto de base la columna 'clean_post' del dataframe de subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57c286",
   "metadata": {},
   "source": [
    "Siguiendo el orden del enunciado, primero se implementará el método basado en TF-IDF + algoritmo de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023022e",
   "metadata": {},
   "source": [
    "**Obtenemos las filas del dataset que no den errores a la hora de normalizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bef982d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.465560600Z",
     "start_time": "2025-01-12T15:41:42.620942200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_df = pd.read_csv('processed_dataset.csv', delimiter=';', quotechar='\"', encoding='utf-8', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "reddit_df.drop(reddit_df.loc[reddit_df.clean_post.isna()].index, inplace=True)\n",
    "reddit_df.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.606719700Z",
     "start_time": "2025-01-12T15:41:51.512369500Z"
    }
   },
   "id": "ced0d9c311e87e16"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         index         created_date created_timestamp  subreddit  \\\n0            0  2010-02-11 19:47:22      1265910442.0  analytics   \n1            1  2010-03-04 20:17:26      1267726646.0  analytics   \n2            2  2011-01-06 04:51:18      1294282278.0  analytics   \n3            3  2011-01-19 11:45:30      1295430330.0  analytics   \n4            4  2011-01-19 21:52:28      1295466748.0  analytics   \n...        ...                  ...               ...        ...   \n272243  274207  2022-05-07 21:38:52      1651948732.0     rstats   \n272244  274208  2022-05-07 22:13:52      1651950832.0     rstats   \n272245  274209  2022-05-08 00:38:50      1651959530.0     rstats   \n272246  274210  2022-05-08 01:19:00      1651961940.0     rstats   \n272247  274211  2022-05-08 01:19:34      1651961974.0     rstats   \n\n                                                    title             author  \\\n0       So what do you guys all do related to analytic...               xtom   \n1       Google's Invasive, non-Anonymized Ad Targeting...               xtom   \n2       DotCed - Functional Web Analytics - Tagging, R...             dotced   \n3                 Program Details - Data Analytics Course      iqrconsulting   \n4       potential job in web analytics... need to anal...   therewontberiots   \n...                                                   ...                ...   \n272243               Help interpretting lmer model output  seeking-stillness   \n272244                          Medical stats book with R  Sweaty_Catch_4275   \n272245        Markov chains with unequal sequence lengths            sebelly   \n272246                   view all available Rcpp::plugins            BOBOLIU   \n272247                    Print only loadings in factanal       artgotframed   \n\n        author_created_utc                                          full_link  \\\n0             1.227476e+09  https://www.reddit.com/r/analytics/comments/b0...   \n1             1.227476e+09  https://www.reddit.com/r/analytics/comments/b9...   \n2             1.294282e+09  https://www.reddit.com/r/analytics/comments/ew...   \n3             1.288245e+09  https://www.reddit.com/r/analytics/comments/f5...   \n4             1.278672e+09  https://www.reddit.com/r/analytics/comments/f5...   \n...                    ...                                                ...   \n272243                 NaN  https://www.reddit.com/r/rstats/comments/ukjiy...   \n272244                 NaN  https://www.reddit.com/r/rstats/comments/ukk7u...   \n272245                 NaN  https://www.reddit.com/r/rstats/comments/ukn1i...   \n272246                 NaN  https://www.reddit.com/r/rstats/comments/uknuh...   \n272247                 NaN  https://www.reddit.com/r/rstats/comments/uknuw...   \n\n        score  num_comments  num_crossposts  subreddit_subscribers  \\\n0         7.0           4.0             0.0                    NaN   \n1         2.0           1.0             0.0                    NaN   \n2         1.0           1.0             NaN                    NaN   \n3         0.0           0.0             NaN                    NaN   \n4         2.0           4.0             NaN                    NaN   \n...       ...           ...             ...                    ...   \n272243    1.0           0.0             0.0                64078.0   \n272244    1.0           0.0             0.0                64080.0   \n272245    1.0           0.0             0.0                64083.0   \n272246    1.0           0.0             0.0                64084.0   \n272247    1.0           0.0             0.0                64084.0   \n\n                                                     post sentiment  \\\n0       There's a lot of reasons to want to know all t...  NEGATIVE   \n1       I'm cross posting this from /r/cyberlaw, hopef...  NEGATIVE   \n2       DotCed,a Functional Analytics Consultant, offe...  NEGATIVE   \n3       Here is the program details of the data analyt...  NEGATIVE   \n4       i decided grad school (physics) was not for me...  POSITIVE   \n...                                                   ...       ...   \n272243  Hello! I am wonder how the following output wo...  NEGATIVE   \n272244  Can anybody recommend me a book with medical s...  POSITIVE   \n272245  I'm trying to build a simple Markov chain. I h...  NEGATIVE   \n272246  How do I view all available Rcpp::plugins? Tha...  POSITIVE   \n272247  Hi everybody,\\n\\nI am currently doing a factor...  NEGATIVE   \n\n        author_created_date                                         clean_post  \n0       2008-11-23 21:27:57  theres lot reasons want know stuff figured id ...  \n1       2008-11-23 21:27:57  im cross posting hopefully guys find interesti...  \n2       2011-01-06 02:49:14  dotceda functional analytics consultant offeri...  \n3       2010-10-28 05:49:49  program details data analytics certification c...  \n4       2010-07-09 10:45:42  decided grad school physics branching job mark...  \n...                     ...                                                ...  \n272243                  NaN  hello wonder following output would interprete...  \n272244                  NaN  anybody recommend book medical statistics r th...  \n272245                  NaN  im trying build simple markov chain data thera...  \n272246                  NaN                  view available rcppplugins thanks  \n272247                  NaN  hi everybody currently factor analysis using f...  \n\n[272248 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>created_date</th>\n      <th>created_timestamp</th>\n      <th>subreddit</th>\n      <th>title</th>\n      <th>author</th>\n      <th>author_created_utc</th>\n      <th>full_link</th>\n      <th>score</th>\n      <th>num_comments</th>\n      <th>num_crossposts</th>\n      <th>subreddit_subscribers</th>\n      <th>post</th>\n      <th>sentiment</th>\n      <th>author_created_date</th>\n      <th>clean_post</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2010-02-11 19:47:22</td>\n      <td>1265910442.0</td>\n      <td>analytics</td>\n      <td>So what do you guys all do related to analytic...</td>\n      <td>xtom</td>\n      <td>1.227476e+09</td>\n      <td>https://www.reddit.com/r/analytics/comments/b0...</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>There's a lot of reasons to want to know all t...</td>\n      <td>NEGATIVE</td>\n      <td>2008-11-23 21:27:57</td>\n      <td>theres lot reasons want know stuff figured id ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2010-03-04 20:17:26</td>\n      <td>1267726646.0</td>\n      <td>analytics</td>\n      <td>Google's Invasive, non-Anonymized Ad Targeting...</td>\n      <td>xtom</td>\n      <td>1.227476e+09</td>\n      <td>https://www.reddit.com/r/analytics/comments/b9...</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>I'm cross posting this from /r/cyberlaw, hopef...</td>\n      <td>NEGATIVE</td>\n      <td>2008-11-23 21:27:57</td>\n      <td>im cross posting hopefully guys find interesti...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2011-01-06 04:51:18</td>\n      <td>1294282278.0</td>\n      <td>analytics</td>\n      <td>DotCed - Functional Web Analytics - Tagging, R...</td>\n      <td>dotced</td>\n      <td>1.294282e+09</td>\n      <td>https://www.reddit.com/r/analytics/comments/ew...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>DotCed,a Functional Analytics Consultant, offe...</td>\n      <td>NEGATIVE</td>\n      <td>2011-01-06 02:49:14</td>\n      <td>dotceda functional analytics consultant offeri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2011-01-19 11:45:30</td>\n      <td>1295430330.0</td>\n      <td>analytics</td>\n      <td>Program Details - Data Analytics Course</td>\n      <td>iqrconsulting</td>\n      <td>1.288245e+09</td>\n      <td>https://www.reddit.com/r/analytics/comments/f5...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Here is the program details of the data analyt...</td>\n      <td>NEGATIVE</td>\n      <td>2010-10-28 05:49:49</td>\n      <td>program details data analytics certification c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2011-01-19 21:52:28</td>\n      <td>1295466748.0</td>\n      <td>analytics</td>\n      <td>potential job in web analytics... need to anal...</td>\n      <td>therewontberiots</td>\n      <td>1.278672e+09</td>\n      <td>https://www.reddit.com/r/analytics/comments/f5...</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>i decided grad school (physics) was not for me...</td>\n      <td>POSITIVE</td>\n      <td>2010-07-09 10:45:42</td>\n      <td>decided grad school physics branching job mark...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>272243</th>\n      <td>274207</td>\n      <td>2022-05-07 21:38:52</td>\n      <td>1651948732.0</td>\n      <td>rstats</td>\n      <td>Help interpretting lmer model output</td>\n      <td>seeking-stillness</td>\n      <td>NaN</td>\n      <td>https://www.reddit.com/r/rstats/comments/ukjiy...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64078.0</td>\n      <td>Hello! I am wonder how the following output wo...</td>\n      <td>NEGATIVE</td>\n      <td>NaN</td>\n      <td>hello wonder following output would interprete...</td>\n    </tr>\n    <tr>\n      <th>272244</th>\n      <td>274208</td>\n      <td>2022-05-07 22:13:52</td>\n      <td>1651950832.0</td>\n      <td>rstats</td>\n      <td>Medical stats book with R</td>\n      <td>Sweaty_Catch_4275</td>\n      <td>NaN</td>\n      <td>https://www.reddit.com/r/rstats/comments/ukk7u...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64080.0</td>\n      <td>Can anybody recommend me a book with medical s...</td>\n      <td>POSITIVE</td>\n      <td>NaN</td>\n      <td>anybody recommend book medical statistics r th...</td>\n    </tr>\n    <tr>\n      <th>272245</th>\n      <td>274209</td>\n      <td>2022-05-08 00:38:50</td>\n      <td>1651959530.0</td>\n      <td>rstats</td>\n      <td>Markov chains with unequal sequence lengths</td>\n      <td>sebelly</td>\n      <td>NaN</td>\n      <td>https://www.reddit.com/r/rstats/comments/ukn1i...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64083.0</td>\n      <td>I'm trying to build a simple Markov chain. I h...</td>\n      <td>NEGATIVE</td>\n      <td>NaN</td>\n      <td>im trying build simple markov chain data thera...</td>\n    </tr>\n    <tr>\n      <th>272246</th>\n      <td>274210</td>\n      <td>2022-05-08 01:19:00</td>\n      <td>1651961940.0</td>\n      <td>rstats</td>\n      <td>view all available Rcpp::plugins</td>\n      <td>BOBOLIU</td>\n      <td>NaN</td>\n      <td>https://www.reddit.com/r/rstats/comments/uknuh...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64084.0</td>\n      <td>How do I view all available Rcpp::plugins? Tha...</td>\n      <td>POSITIVE</td>\n      <td>NaN</td>\n      <td>view available rcppplugins thanks</td>\n    </tr>\n    <tr>\n      <th>272247</th>\n      <td>274211</td>\n      <td>2022-05-08 01:19:34</td>\n      <td>1651961974.0</td>\n      <td>rstats</td>\n      <td>Print only loadings in factanal</td>\n      <td>artgotframed</td>\n      <td>NaN</td>\n      <td>https://www.reddit.com/r/rstats/comments/uknuw...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64084.0</td>\n      <td>Hi everybody,\\n\\nI am currently doing a factor...</td>\n      <td>NEGATIVE</td>\n      <td>NaN</td>\n      <td>hi everybody currently factor analysis using f...</td>\n    </tr>\n  </tbody>\n</table>\n<p>272248 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.684813200Z",
     "start_time": "2025-01-12T15:41:51.606719700Z"
    }
   },
   "id": "60439b613845fa41"
  },
  {
   "cell_type": "markdown",
   "id": "03288378",
   "metadata": {},
   "source": [
    "**Las dos columnas que necesitaremos para llevar a cabo el aprendizaje supervisado son 'clean_post' y 'subreddit'.** \\\n",
    "Por ello, limitamos el dataset a estas dos columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d92985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.731666200Z",
     "start_time": "2025-01-12T15:41:51.669228Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[['clean_post', 'subreddit']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb213b",
   "metadata": {},
   "source": [
    "**Dividimos el conjunto de datos en conjuntos de entrenamiento y prueba (70% para entrenamiento, 30% para prueba).** \\ Usamos la función train_test_split de sklearn.model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d811f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.934789600Z",
     "start_time": "2025-01-12T15:41:51.716041400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = reddit_df['clean_post']\n",
    "y = reddit_df['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.series.Series"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:41:51.966038100Z",
     "start_time": "2025-01-12T15:41:51.809813900Z"
    }
   },
   "id": "bb196daab62de0a4"
  },
  {
   "cell_type": "markdown",
   "id": "27886cf2",
   "metadata": {},
   "source": [
    "**2.1. TF-IDF + algoritmo de machine learning** \\\n",
    "Utilizaremos la representación TF-IDF para transformar el texto y un clasificador de Regresión Logística para la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c10aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T15:43:51.965219500Z",
     "start_time": "2025-01-12T15:41:51.825408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5114427677633718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:                       precision    recall  f1-score   support\n",
      "\n",
      "       AskStatistics       0.50      0.58      0.54      9055\n",
      "     DataScienceJobs       0.84      0.60      0.70       688\n",
      "         MLQuestions       0.20      0.04      0.07      3410\n",
      "     MachineLearning       0.45      0.58      0.51     11223\n",
      "           analytics       0.76      0.58      0.66      2349\n",
      "          artificial       0.58      0.40      0.48      2621\n",
      "     computerscience       0.64      0.80      0.71      6711\n",
      "      computervision       0.62      0.53      0.57      2925\n",
      "                data       0.72      0.21      0.33       799\n",
      "        dataanalysis       0.48      0.11      0.18      1214\n",
      "     dataengineering       0.76      0.64      0.70      2468\n",
      "         datascience       0.56      0.64      0.59     11171\n",
      "  datascienceproject       0.00      0.00      0.00        75\n",
      "            datasets       0.61      0.70      0.65      3440\n",
      "        deeplearning       0.32      0.09      0.14      2432\n",
      "              kaggle       0.50      0.08      0.14       126\n",
      "learnmachinelearning       0.42      0.48      0.45      8763\n",
      "              rstats       0.67      0.58      0.62      3252\n",
      "          statistics       0.44      0.42      0.43      8953\n",
      "\n",
      "            accuracy                           0.53     81675\n",
      "           macro avg       0.53      0.42      0.45     81675\n",
      "        weighted avg       0.52      0.53      0.51     81675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Creamos el pipeline\n",
    "tfidf_lr_pipeline = Pipeline([('vectorizer', TfidfVectorizer()), ('logistic', LogisticRegression())])\n",
    "tfidf_lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_tfidf_lr = tfidf_lr_pipeline.predict(X_test)\n",
    "\n",
    "f1_score_tfidf_lr = f1_score(y_test, y_pred_tfidf_lr, average='weighted')\n",
    "print(f'F1 score: {f1_score_tfidf_lr}')\n",
    "\n",
    "print(f'Classification report: {classification_report(y_test, y_pred_tfidf_lr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['MachineLearning', 'datascience'], dtype=object)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_lr_pipeline.predict(pd.Series(['Machine Learning is the subject that studies different ways for machine to have what is called intelligent behavior', 'I have found a great job in data science, I hope you like it']))                                                       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:43:52.074644600Z",
     "start_time": "2025-01-12T15:43:51.965219500Z"
    }
   },
   "id": "f5c2873222afd251"
  },
  {
   "cell_type": "markdown",
   "id": "e1ee86be",
   "metadata": {},
   "source": [
    "**2.2. Word Embeddings + Logistic Regression** "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos las librerías necesarias, y extraemos unas muestras para entrenamiento y testing para poder ejecutar el código en un tiempo razonable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c2cecb2b1c497"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "X_train_emb = X_train.sample(frac=0.1, random_state=42)\n",
    "y_train_emb = y_train.loc[X_train_emb.index]\n",
    "X_test_emb = X_test.sample(frac=0.01, random_state=42)\n",
    "y_test_emb = y_test.loc[X_test_emb.index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:12:23.155301600Z",
     "start_time": "2025-01-12T16:12:23.045892700Z"
    }
   },
   "id": "873193e0d3a358f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para la formación de los embeddins usaremos el modelo distilbert, cuya arquitectura deriva del modelo Bert, pero más simplificado, haciéndolo mucho más rápido sacrificando precisión. Usaremos un modelo preentrenado de la librería transformers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ed665ea55db904"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Inicializar DistilBERT y el tokenizador\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:44:09.021461200Z",
     "start_time": "2025-01-12T15:43:52.181513700Z"
    }
   },
   "id": "869177773089a149"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos la una clase que herede de las clases BaseEstimator y TransformerMixin de la librería sklearn, para que el modelo sea compatible con los pipeline de sklearn."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59e7d837411f6394"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class DistilBERTEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No se necesita ajuste en este transformador\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for text in X:\n",
    "                inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                outputs = self.model(**inputs)\n",
    "                token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "                word_embedding = token_embeddings.mean(dim=0).numpy()\n",
    "                embeddings.append(word_embedding)\n",
    "        return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T15:44:09.037022800Z",
     "start_time": "2025-01-12T15:44:09.021461200Z"
    }
   },
   "id": "695631d635bbce86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos el modelo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc27083706ce3aa"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('vectorizer', DistilBERTEmbeddingTransformer()),\n                ('logistic', LogisticRegression())])",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, DistilBERTEmbeddingTransformer()),\n                (&#x27;logistic&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, DistilBERTEmbeddingTransformer()),\n                (&#x27;logistic&#x27;, LogisticRegression())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">DistilBERTEmbeddingTransformer</label><div class=\"sk-toggleable__content fitted\"><pre>DistilBERTEmbeddingTransformer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_lr_pipeline = Pipeline([('vectorizer',  DistilBERTEmbeddingTransformer()), ('logistic', LogisticRegression())])\n",
    "embedding_lr_pipeline.fit(X_train_emb, y_train_emb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:03:34.464341700Z",
     "start_time": "2025-01-12T15:44:09.037022800Z"
    }
   },
   "id": "98f8ac7f15a09504"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos las predicciones con el conjunte de testeo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dda6d060b08c7b7"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "y_pred_embedding_lr = embedding_lr_pipeline.predict(X_test_emb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:13:25.499014700Z",
     "start_time": "2025-01-12T16:12:29.651294700Z"
    }
   },
   "id": "93a413f1856467b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos las pruebas de f1_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28b16d6fb615a637"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.4292594966120053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:                       precision    recall  f1-score   support\n",
      "\n",
      "       AskStatistics       0.50      0.58      0.54      9055\n",
      "     DataScienceJobs       0.84      0.60      0.70       688\n",
      "         MLQuestions       0.20      0.04      0.07      3410\n",
      "     MachineLearning       0.45      0.58      0.51     11223\n",
      "           analytics       0.76      0.58      0.66      2349\n",
      "          artificial       0.58      0.40      0.48      2621\n",
      "     computerscience       0.64      0.80      0.71      6711\n",
      "      computervision       0.62      0.53      0.57      2925\n",
      "                data       0.72      0.21      0.33       799\n",
      "        dataanalysis       0.48      0.11      0.18      1214\n",
      "     dataengineering       0.76      0.64      0.70      2468\n",
      "         datascience       0.56      0.64      0.59     11171\n",
      "  datascienceproject       0.00      0.00      0.00        75\n",
      "            datasets       0.61      0.70      0.65      3440\n",
      "        deeplearning       0.32      0.09      0.14      2432\n",
      "              kaggle       0.50      0.08      0.14       126\n",
      "learnmachinelearning       0.42      0.48      0.45      8763\n",
      "              rstats       0.67      0.58      0.62      3252\n",
      "          statistics       0.44      0.42      0.43      8953\n",
      "\n",
      "            accuracy                           0.53     81675\n",
      "           macro avg       0.53      0.42      0.45     81675\n",
      "        weighted avg       0.52      0.53      0.51     81675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bakal\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "f1_score_tfidf_lr = f1_score(y_test_emb, y_pred_embedding_lr, average='weighted')\n",
    "print(f'F1 score: {f1_score_tfidf_lr}')\n",
    "\n",
    "print(f'Classification report: {classification_report(y_test, y_pred_tfidf_lr)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:15:06.150212500Z",
     "start_time": "2025-01-12T16:15:04.105638600Z"
    }
   },
   "id": "acf6951645a67d70"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19057 19057\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_emb), len(y_train_emb))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:15:19.264968200Z",
     "start_time": "2025-01-12T16:15:19.186789600Z"
    }
   },
   "id": "ea2f1e9c05d3217f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "421c07008c671fa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3. NER (Reconocimiento de Entidades Nombradas) + Random Forest**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84a7ddd91591f434"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importamos las bibliotecas necesarias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef29c6d0a026fdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.383801400Z"
    }
   },
   "id": "57d87abd01890e03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Leemos el archivo clean_post, nos quedamos con las dos columnas que nos interesan (clean_post y subreddit), y cargamos el modelo de lenguaje en_core_web_sm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2cd22f112787a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "reddit_df = pd.read_csv('processed_dataset.csv', delimiter=';', quotechar='\"', encoding='utf-8', low_memory=False)\n",
    "reddit_df.drop(reddit_df.loc[reddit_df.clean_post.isna()].index, inplace=True)\n",
    "reddit_df.reset_index(inplace=True)\n",
    "reddit_df = reddit_df[['clean_post', 'subreddit']]\n",
    "\n",
    "# Carga del modelo de SpaCy para NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.383801400Z"
    }
   },
   "id": "edf3eeb24ef2181e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos una función que extraiga las entidades reconocidas en un texto dado."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2644d456d3af58a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Función para extraer entidades nombradas de un texto\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([ent.text for ent in doc.ents])  # Concatenamos las entidades reconocidas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.399433600Z"
    }
   },
   "id": "3ad5e2676bcab0fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reducimos el tamaño del dataset por razones de complejidad temporal, para que el proceso de transformación de los textos y entrenamiento se haga en un márgen de tiempo razonable. Se puede modificar el porcentaje del dataset usado modificando el parámetro frac en reddit.df.sample(frac=0.01)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dbb91e9d1a91608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit_df_sm = reddit_df.sample(frac=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.399433600Z"
    }
   },
   "id": "8d17e84e8a136c4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos una nueva columna entinties, que guardará las entidades reconocidas de cada texto."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e507223325c005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extraer entidades nombradas\n",
    "reddit_df_sm[\"entities\"] = reddit_df_sm[\"clean_post\"].apply(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.399433600Z"
    }
   },
   "id": "af547c996227993a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73968794683ecc16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separamos los sets de entrenamiento y test."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1309896a3a8676bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Usar entidades como característica\n",
    "X = reddit_df_sm[\"entities\"]\n",
    "y = reddit_df_sm[\"subreddit\"]\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.415049100Z"
    }
   },
   "id": "9860a9ed42cb4471"
  },
  {
   "cell_type": "markdown",
   "source": [
    "creamos el pipeline para el entrenamiento, con los objetos de las clases CountVectorizer() para convertir los strings de la columna entinties a valores numéricos que pasar a RandomForestClassifier(), que será el modelo a ajustar."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449d5a2862a5bd6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pipeline de vectorización y clasificación\n",
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(),  # Vectorizamos las entidades extraídas\n",
    "    RandomForestClassifier(random_state=42)  # Modelo de clasificación\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.415049100Z"
    }
   },
   "id": "42a9523e3e886ab8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entrenamos y evaluamos el modelo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc5f113d3271df7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Función de predicción\n",
    "def predict_subreddit_ner(text):\n",
    "    entities = extract_entities(text)\n",
    "    return pipeline.predict([entities])[0]\n",
    "\n",
    "\n",
    "# Prueba de la función\n",
    "test_post = \"I visited Paris last summer\"\n",
    "predicted_subreddit = predict_subreddit_ner(test_post)\n",
    "print(f\"El subreddit predicho es: {predicted_subreddit}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-12T16:12:01.415049100Z"
    }
   },
   "id": "cb49807c116d866c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "afa6651c636bb5f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4. Función classify_subreddit**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f12b52797127e427"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para la función classify_subreddit elegiremos el primero modelo, dado que con un 0.51 es el que mejor f1_score da."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42af15951023f70e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def classify_subreddit(text):\n",
    "    return tfidf_lr_pipeline.predict(pd.Series([text]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T16:19:48.596183300Z",
     "start_time": "2025-01-12T16:19:48.564139800Z"
    }
   },
   "id": "5d22824ec4be9745"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
